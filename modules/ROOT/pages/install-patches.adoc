= Perform Scheduled Maintenance
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

Use the following procedure when performing scheduled maintenance for Anypoint Runtime Fabric nodes, such as applying OS patches and security updates. 

== Controller Node Maintenance

The controller nodes are the entry points into your Runtime Fabric cluster, but do not run any application workloads.

To update controller nodes:

. On your upstream TCP load balancer, remove the controller nodes from your load balancing pool, one at a time, during patching and rebooting tasks. 
. When updates are complete on each controller node, add the node back to the cluster before removing the next one.

== Worker Node Maintenance

To update worker nodes: 

. On a controller node, run the `gravity shell` command.
. Verify that the status for all nodes is `READY`:
+
```
$ sudo kubectl get nodes

NAME          STATUS   ROLES    AGE   VERSION
172.31.0.9    Ready    node     1d   v1.x.x
172.31.0.8    Ready    master   1d   v1.x.x
```
. Retrieve the application namespace (environment ID): 
+
```
$ sudo kubectl get ns
```
+
In the command output, the application namespace is represented by a long string of characters. In the following example, the application namespace
is `fff5df7b-c49d-48c9-967d-0071412722x4`.
+
More than one application namespace value is displayed if the RTF cluster is associated with more than 1 environment.
+
```
NAME                                   STATUS   AGE
default                                Active   1d
fff5df7b-c49d-48c9-967d-0071412722x4   Active   1d
kube-node-lease                        Active   1d
kube-public                            Active   1d
kube-system                            Active   1d
monitoring                             Active   1d
rtf                                    Active   1d
```
. Verify that the pods and nodes are still running in your namespace:
+
```
$ sudo kubectl -n <namespace> get pod -owide
```
+
In the command output, verify that `Running` is displayed in the `READY` column, as shown in the following example:
+
```
NAME                                         READY   STATUS    RESTARTS   AGE     IP             NODE          NOMINATED NODE   READINESS GATES
my-app-name-1-84f95cb7c9-glktk     2/2     Running   0          9m27s   10.244.88.29   172.31.0.9   <none>           <none>
```
+
If you have multiple environments, please iterate in each environment. 
+
. Perform the remaining steps on all worker nodes that require updates:

.. Drain the node. The following actions are performed when a node is drained:

... The application is scheduled on another node and is then deleted.
... Node status is updated to `SchedulingDisabled`. 
+
[IMPORTANT]
Unless you are running at least 2 replicas and enabling "Enforce deploying replicas across node" of your application, service is disrupted during the drain process.
+
```
$ sudo kubectl drain <node name> --ignore-daemonsets --delete-local-data
```

.. Verify that the node's status is `Ready,SchedulingDisabled`:
+
```
$ sudo kubectl get node

NAME          STATUS                     ROLES    AGE   VERSION
172.31.0.9   Ready,SchedulingDisabled   node     1d   v1.x.x
```

.. Verify that no pods are running on the node:
+
```
$ sudo kubectl get pod -n <namespace>  -owide
```
+
The `NODE` column <check the NODE column. There should be no application pods running on the drained node>

.. Proceed with OS patching and reboot the node.

.. After the reboot is complete, re-enable scheduling on the node:
+
```
$ sudo kubectl uncordon 172.31.0.9

node/172.31.0.9 uncordoned

$ sudo kubectl get node

NAME          STATUS   ROLES    AGE   VERSION
172.31.0.9   Ready    node     1d   v1.x.x
```
